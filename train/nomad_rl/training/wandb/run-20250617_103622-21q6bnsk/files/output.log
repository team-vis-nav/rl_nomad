/home/tuandang/miniconda3/envs/nomad_train/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Starting training for 1000000 timesteps...
Device: cuda
Scene names: ['FloorPlan1']

--- Update 10 (Timesteps: 20480) ---
Episode Reward: 94.81 ± 137.90
Episode Length: 437.3
Success Rate: 62.79%
Goal-Conditioned Reward: 6.75
Exploration Reward: 229.48
Policy Loss: -0.0013
Value Loss: 32.9650
Entropy: 1.3694
Approx KL: 0.0076

--- Update 20 (Timesteps: 40960) ---
Episode Reward: 115.64 ± 144.46
Episode Length: 447.4
Success Rate: 67.47%
Goal-Conditioned Reward: 6.29
Exploration Reward: 245.15
Policy Loss: -0.0052
Value Loss: 37.9753
Entropy: 1.2684
Approx KL: 0.0137

--- Update 30 (Timesteps: 61440) ---
Episode Reward: 117.88 ± 150.96
Episode Length: 446.4
Success Rate: 68.00%
Goal-Conditioned Reward: -1.06
Exploration Reward: 253.97
Policy Loss: -0.0018
Value Loss: 69.8391
Entropy: 1.2536
Approx KL: 0.0119

--- Update 40 (Timesteps: 81920) ---
Episode Reward: 138.11 ± 164.83
Episode Length: 469.7
Success Rate: 67.00%
Goal-Conditioned Reward: -4.68
Exploration Reward: 260.89
Policy Loss: -0.0005
Value Loss: 78.7743
Entropy: 1.2209
Approx KL: 0.0089

--- Update 50 (Timesteps: 102400) ---
Episode Reward: 140.09 ± 172.64
Episode Length: 457.5
Success Rate: 69.00%
Goal-Conditioned Reward: -1.80
Exploration Reward: 263.52
Policy Loss: -0.0043
Value Loss: 67.3613
Entropy: 1.0377
Approx KL: 0.0124
Model saved to ./checkpoints/nomad_rl/nomad_rl_50.pth

--- Update 60 (Timesteps: 122880) ---
Episode Reward: 144.27 ± 166.28
Episode Length: 420.3
Success Rate: 76.00%
Goal-Conditioned Reward: 2.12
Exploration Reward: 275.87
Policy Loss: -0.0015
Value Loss: 148.1054
Entropy: 1.0830
Approx KL: 0.0123

--- Update 70 (Timesteps: 143360) ---
Episode Reward: 120.04 ± 154.36
Episode Length: 424.6
Success Rate: 71.00%
Goal-Conditioned Reward: 3.12
Exploration Reward: 272.25
Policy Loss: 0.0008
Value Loss: 74.1497
Entropy: 1.3022
Approx KL: 0.0068
Traceback (most recent call last):
  File "nomad_rl_trainer.py", line 349, in <module>
    main()
  File "nomad_rl_trainer.py", line 346, in main
    trainer.train(config['total_timesteps'])
  File "nomad_rl_trainer.py", line 270, in train
    update_stats = self.update_policy()
  File "nomad_rl_trainer.py", line 238, in update_policy
    self.optimizer.step()
  File "/home/tuandang/miniconda3/envs/nomad_train/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/tuandang/miniconda3/envs/nomad_train/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/tuandang/miniconda3/envs/nomad_train/lib/python3.8/site-packages/torch/optim/adam.py", line 216, in step
    has_complex = self._init_group(
  File "/home/tuandang/miniconda3/envs/nomad_train/lib/python3.8/site-packages/torch/optim/adam.py", line 170, in _init_group
    exp_avg_sqs.append(state["exp_avg_sq"])
KeyError: <list_iterator object at 0x73c8e56f1d00>
