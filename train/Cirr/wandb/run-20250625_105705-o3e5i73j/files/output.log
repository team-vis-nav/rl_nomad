Loaded splits from /home/tuandang/tuandang/quanganh/visualnav-transformer/train/Unified/config/splits/combined_splits.yaml
Dataset: combined
Train scenes: 156
Val scenes: 27
Test scenes: 26
Starting unified training with curriculum learning for 1000000 timesteps...
Dataset: combined
Stage: 1
Using single environment instance to avoid Unity conflicts
torch obs:  {'rgb': tensor([[[[0.5020, 0.4863, 0.5137,  ..., 0.4510, 0.4588, 0.4510],
          [0.5020, 0.4863, 0.5137,  ..., 0.4471, 0.4588, 0.4510],
          [0.5059, 0.4667, 0.5020,  ..., 0.4471, 0.4549, 0.4549],
          ...,
          [0.1059, 0.1059, 0.1059,  ..., 0.5059, 0.5255, 0.5176],
          [0.1059, 0.1059, 0.1059,  ..., 0.5098, 0.5098, 0.4902],
          [0.1020, 0.1059, 0.1059,  ..., 0.5216, 0.4980, 0.4392]],
         [[0.2824, 0.2706, 0.2941,  ..., 0.2431, 0.2549, 0.2431],
          [0.2824, 0.2706, 0.2941,  ..., 0.2392, 0.2549, 0.2431],
          [0.2863, 0.2627, 0.2824,  ..., 0.2392, 0.2510, 0.2510],
          ...,
          [0.1020, 0.0980, 0.1020,  ..., 0.4902, 0.5176, 0.5020],
          [0.0980, 0.0980, 0.1020,  ..., 0.4902, 0.4941, 0.4706],
          [0.0980, 0.0980, 0.1020,  ..., 0.5059, 0.4824, 0.4196]],
         [[0.1490, 0.1529, 0.1569,  ..., 0.1255, 0.1255, 0.1255],
          [0.1529, 0.1490, 0.1529,  ..., 0.1216, 0.1255, 0.1255],
          [0.1529, 0.1451, 0.1529,  ..., 0.1216, 0.1255, 0.1255],
          ...,
          [0.0824, 0.0824, 0.0824,  ..., 0.4510, 0.4745, 0.4627],
          [0.0824, 0.0824, 0.0824,  ..., 0.4510, 0.4510, 0.4353],
          [0.0784, 0.0824, 0.0824,  ..., 0.4745, 0.4392, 0.3765]]]],
       device='cuda:0'), 'goal_rgb': tensor([[[[0.6000, 0.6000, 0.6000,  ..., 0.6078, 0.6078, 0.6078],
          [0.5961, 0.6000, 0.6000,  ..., 0.6078, 0.6078, 0.6078],
          [0.5961, 0.6000, 0.5961,  ..., 0.6078, 0.6078, 0.6078],
          ...,
          [0.6471, 0.6353, 0.6353,  ..., 0.5569, 0.5569, 0.5608],
          [0.6118, 0.6157, 0.6157,  ..., 0.5569, 0.5569, 0.5569],
          [0.5647, 0.5647, 0.5647,  ..., 0.5608, 0.5608, 0.5569]],
         [[0.5725, 0.5725, 0.5765,  ..., 0.5725, 0.5765, 0.5725],
          [0.5725, 0.5725, 0.5725,  ..., 0.5804, 0.5765, 0.5725],
          [0.5725, 0.5725, 0.5725,  ..., 0.5804, 0.5725, 0.5725],
          ...,
          [0.3922, 0.3922, 0.4000,  ..., 0.5373, 0.5373, 0.5333],
          [0.3843, 0.3843, 0.3843,  ..., 0.5373, 0.5373, 0.5333],
          [0.3490, 0.3529, 0.3529,  ..., 0.5373, 0.5373, 0.5373]],
         [[0.4941, 0.4980, 0.4941,  ..., 0.4902, 0.4902, 0.4902],
          [0.4980, 0.4941, 0.4980,  ..., 0.4902, 0.4902, 0.4902],
          [0.4980, 0.4980, 0.4980,  ..., 0.4902, 0.4902, 0.4902],
          ...,
          [0.2000, 0.1961, 0.1961,  ..., 0.4627, 0.4627, 0.4627],
          [0.1961, 0.1961, 0.1961,  ..., 0.4627, 0.4627, 0.4627],
          [0.1882, 0.1882, 0.1922,  ..., 0.4627, 0.4627, 0.4627]]]],
       device='cuda:0'), 'context': tensor([[[[0.5020, 0.4863, 0.5137,  ..., 0.4510, 0.4588, 0.4510],
          [0.5020, 0.4863, 0.5137,  ..., 0.4471, 0.4588, 0.4510],
          [0.5059, 0.4667, 0.5020,  ..., 0.4471, 0.4549, 0.4549],
          ...,
          [0.1059, 0.1059, 0.1059,  ..., 0.5059, 0.5255, 0.5176],
          [0.1059, 0.1059, 0.1059,  ..., 0.5098, 0.5098, 0.4902],
          [0.1020, 0.1059, 0.1059,  ..., 0.5216, 0.4980, 0.4392]],
         [[0.2824, 0.2706, 0.2941,  ..., 0.2431, 0.2549, 0.2431],
          [0.2824, 0.2706, 0.2941,  ..., 0.2392, 0.2549, 0.2431],
          [0.2863, 0.2627, 0.2824,  ..., 0.2392, 0.2510, 0.2510],
          ...,
          [0.1020, 0.0980, 0.1020,  ..., 0.4902, 0.5176, 0.5020],
          [0.0980, 0.0980, 0.1020,  ..., 0.4902, 0.4941, 0.4706],
          [0.0980, 0.0980, 0.1020,  ..., 0.5059, 0.4824, 0.4196]],
         [[0.1490, 0.1529, 0.1569,  ..., 0.1255, 0.1255, 0.1255],
          [0.1529, 0.1490, 0.1529,  ..., 0.1216, 0.1255, 0.1255],
          [0.1529, 0.1451, 0.1529,  ..., 0.1216, 0.1255, 0.1255],
          ...,
          [0.0824, 0.0824, 0.0824,  ..., 0.4510, 0.4745, 0.4627],
          [0.0824, 0.0824, 0.0824,  ..., 0.4510, 0.4510, 0.4353],
          [0.0784, 0.0824, 0.0824,  ..., 0.4745, 0.4392, 0.3765]],
         ...,
         [[0.5020, 0.4863, 0.5137,  ..., 0.4510, 0.4588, 0.4510],
          [0.5020, 0.4863, 0.5137,  ..., 0.4471, 0.4588, 0.4510],
          [0.5059, 0.4667, 0.5020,  ..., 0.4471, 0.4549, 0.4549],
          ...,
          [0.1059, 0.1059, 0.1059,  ..., 0.5059, 0.5255, 0.5176],
          [0.1059, 0.1059, 0.1059,  ..., 0.5098, 0.5098, 0.4902],
          [0.1020, 0.1059, 0.1059,  ..., 0.5216, 0.4980, 0.4392]],
         [[0.2824, 0.2706, 0.2941,  ..., 0.2431, 0.2549, 0.2431],
          [0.2824, 0.2706, 0.2941,  ..., 0.2392, 0.2549, 0.2431],
          [0.2863, 0.2627, 0.2824,  ..., 0.2392, 0.2510, 0.2510],
          ...,
          [0.1020, 0.0980, 0.1020,  ..., 0.4902, 0.5176, 0.5020],
          [0.0980, 0.0980, 0.1020,  ..., 0.4902, 0.4941, 0.4706],
          [0.0980, 0.0980, 0.1020,  ..., 0.5059, 0.4824, 0.4196]],
         [[0.1490, 0.1529, 0.1569,  ..., 0.1255, 0.1255, 0.1255],
          [0.1529, 0.1490, 0.1529,  ..., 0.1216, 0.1255, 0.1255],
          [0.1529, 0.1451, 0.1529,  ..., 0.1216, 0.1255, 0.1255],
          ...,
          [0.0824, 0.0824, 0.0824,  ..., 0.4510, 0.4745, 0.4627],
          [0.0824, 0.0824, 0.0824,  ..., 0.4510, 0.4510, 0.4353],
          [0.0784, 0.0824, 0.0824,  ..., 0.4745, 0.4392, 0.3765]]]],
       device='cuda:0'), 'goal_mask': tensor([[0.]], device='cuda:0'), 'goal_position': tensor([[-1.2500,  0.9090,  3.5000]], device='cuda:0')}
input obs shape: torch.Size([1, 15, 224, 224])
input goal shape: torch.Size([1, 6, 224, 224])
shape after effnet: torch.Size([5, 1280, 7, 7])
shape after avgpooling: torch.Size([5, 1280, 1, 1])
shape after flatten: torch.Size([5, 1280])
Error in vision encoder: shape '[6, -1, 256]' is invalid for input of size 1280
obs_img shape: torch.Size([1, 15, 224, 224])
obsgoal_img shape: torch.Size([1, 6, 224, 224])
goal_mask shape: torch.Size([1, 1])
Traceback (most recent call last):
  File "Train.py", line 727, in <module>
    main()
  File "Train.py", line 706, in main
    results = trainer.train_val_test(config[f'stage{args.stage}_timesteps'])
  File "Train.py", line 411, in train_val_test
    rollout_stats = self.collect_rollouts(self.config['rollout_steps'])
  File "Train.py", line 218, in collect_rollouts
    print("torch obs: ", next_torch_obs.shape)
AttributeError: 'dict' object has no attribute 'shape'