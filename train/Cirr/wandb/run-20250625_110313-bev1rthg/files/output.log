Loaded splits from /home/tuandang/tuandang/quanganh/visualnav-transformer/train/Unified/config/splits/combined_splits.yaml
Dataset: combined
Train scenes: 156
Val scenes: 27
Test scenes: 26
Starting unified training with curriculum learning for 1000000 timesteps...
Dataset: combined
Stage: 1
Using single environment instance to avoid Unity conflicts
torch obs:  torch.Size([1, 3, 224, 224])
input obs shape: torch.Size([1, 15, 224, 224])
input goal shape: torch.Size([1, 6, 224, 224])
shape after effnet: torch.Size([5, 1280, 7, 7])
shape after avgpooling: torch.Size([5, 1280, 1, 1])
shape after flatten: torch.Size([5, 1280])
Error in vision encoder: shape '[6, -1, 256]' is invalid for input of size 1280
obs_img shape: torch.Size([1, 15, 224, 224])
obsgoal_img shape: torch.Size([1, 6, 224, 224])
goal_mask shape: torch.Size([1, 1])
torch obs:  torch.Size([1, 3, 224, 224])
Traceback (most recent call last):
  File "Train.py", line 727, in <module>
    main()
  File "Train.py", line 706, in main
    results = trainer.train_val_test(config[f'stage{args.stage}_timesteps'])
  File "Train.py", line 411, in train_val_test
    rollout_stats = self.collect_rollouts(self.config['rollout_steps'])
  File "Train.py", line 187, in collect_rollouts
    if torch_obs[key].dim() == 3 and key in ['rgb', 'goal_rgb', 'context']:
IndexError: tensors used as indices must be long, byte or bool tensors